{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning from Imbalanced Data\n",
    "### Haibo He, Member, IEEE, and Edwardo A. Garcia\n",
    "###### IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 21, NO. 9, SEPTEMBER 2009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Imbalanced Data: Underrepresented Data and severe class distribution skews\n",
    "\n",
    "- Most standard algorithms assume or expect balanced class distributions or equal misclassification costs. Therefore, when presented with complex imbalanced data sets, these algorithms fail to properly represent the distributive characteristics of the data and resultantly provide unfavorable accuracies across the classes of the data.\n",
    "\n",
    "- Some of the methods targeting the imbalanced learning -\n",
    "    * Sampling methods\n",
    "    * Cost-Sensitive learning methods\n",
    "    * Kernel-based learning methods\n",
    "    * Active Learning method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE::** *This paper briefly touched upon the multiclass imbalanced learning problem, focusing instead on the two-class imbalanced learning problem for space considerations.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- More informative assessment metrics, such as the ROC curves, precision-recall curves, and cost curves, are necessary for conclusive evaluations of performance in the presence of imbalanced data.\n",
    "\n",
    "- **Instrinsic Variety:** Imbalances in data as a direct result of the nature of the dataspace.\n",
    "- **Exstrinsic Variety:** Imbalances in data is not directly related to the nature of the dataspace. Time and storage can result in imbalanced data.\n",
    "\n",
    "- **Relative Imbalance:** Imabalances in which minority class is in significant number but is rare relative to majority class.\n",
    "- **Imbalance due to rare instances (or, absoulte rarity)**\n",
    "\n",
    "- **Between-Class Imbalance:**\n",
    "- **Within-Class Imbalance:** \n",
    "    * Minority/Majority concept may additionally contain a subconcept with limited instances, amounting to diverging degrees of classification difficulty and leading to within-class imbalance.\n",
    "    * It concerns itself with the distribution of representative data for subconcepts within a class.\n",
    "    * The no. of examples in the dominant clusters significantly outnumbers the examples in their respective subconcept clusters. \n",
    "    * Existence of within-class imbalances is closely interwined with the problem of small disjuncts, which has been shown to greatly depricate classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A classifier will attempt to learn a concept by creating multiple disjunct rules that describe the main concept.\n",
    "- In the case of homogeneous concepts, the classifier will generally create large disjuncts, i.e., rules that cover a large portion (cluster) of examples pertaining to the main concept.\n",
    "- However, in the case of heterogeneous concepts, small disjuncts, i.e., rules that cover a small cluster of examples pertaining to the main concept, arise as a direct result of underrepresented subconcepts.\n",
    "- Moreover, since classifiers attempt to learn both majority and minority concepts, the problem of small disjuncts is not only restricted to the minority concept.\n",
    "- On the contrary, small disjuncts of the majority class can arise from noisy misclassified minority class examples or underrepresented subconcepts.\n",
    "- However, because of the vast representation of majority class data, this occurrence is infrequent.\n",
    "- A more common scenario is that noise may influence disjuncts in the minority class, questioning whether these examples represent an actual subconcept or are merely attributed to noise, thus asking for the validity of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When standard learning algorithms are applied to imbalanced data, the induction rules that describe the minority concepts are often fewer and weaker than those of majority concepts, since the minority class is often both outnumbered and under-represented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decision trees use a recursive, top-down greedy search algorithm that uses a feature selection scheme (e.g., information gain) to select the best feature as the split criterion at each node of the tree; a successor (leaf) is then created for each of the possible values corresponding to the split feature.\n",
    "- The problem with decision tree algorithms in the presence of imbalanced data is\n",
    "    * First, successive partitioning of the dataspace results in fewer and fewer observations of minority class examples resulting in fewer leaves describing the minority concepts and successively weaker confidence estimates.\n",
    "    * Second, concepts that have dependencies on different feature space conjugations can go unlearned by the sparseness intorduced through partitioning.\n",
    "- First issue correlates with the problems of relative and absolute imbalances, while the second issue best correlates with the between-class imbalance and the problem of high dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Studies have shown that for several base classifiers, a balanced data set provides improved overall classification performance compared to an imbalanced data set.\n",
    "- However, they do not imply that classifiers cannot learn from imbalanced data sets; on the contrary, studies have also shown that classifiers induced from certain imbalanced data sets are comparable to classifiers induced from the same data set balanced by sampling techniques.\n",
    "- Nevertheless, for most imbalanced data sets, the application of sampling techniques does indeed aid in improved classifier accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Random Oversampling and Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Undersampling aims to balance class distribution by randomly eliminating majority class examples.\n",
    "- Advantages:\n",
    "    * It can help improve run time and storage problems by reducing the number of training data samples when the training data set is huge.\n",
    "- Disadvantages:\n",
    "    * It can discard potentially useful information which could be important for building rule classifiers.\n",
    "    * The sample chosen by random under sampling may be a biased sample. And it will not be an accurate representative of the population. Thereby, resulting in inaccurate results with the actual test data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Oversampling increases the number of instances in the minority class by randomly replicating them in order to present a higher representation of the minority class in the sample.\n",
    "- Advantages:\n",
    "    * Unlike under sampling this method leads to no information loss.\n",
    "    * Outperforms under sampling\n",
    "- Disadvantages:\n",
    "    * It increases the likelihood of overfitting since it replicates the minority class events. <br>Oversampling simply appends replicated data to the original data set, resulting in multiple instances of certain examples becoming “tied”, leading to overfitting.<br>In particular, overfitting in oversampling occurs when classifiers produce multiple clauses in a rule for multiple copies of the same example which causes the rule to become too specific; although the training accuracy will be high in this scenario, the classification performance on the unseen testing data is generally far worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Informed Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Two examples of informed undersampling that have shown good results are **EasyEnsemble** and **BalanceCascade** algorithms.\n",
    "- The objective of these two methods is to overcome the deficiency of information loss introduced in the traditional random undersampling method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 EasyEnsemble\n",
    "\n",
    "- Implementation of EasyEnsemble is straightforward. It develops an ensemble learning system by independently sampling several subsets from the majority class and developing multiple classifiers based on the combination of each subset with the minority class data.\n",
    "- EasyEnsemble can be considered as an unsupervised learning that explores the majority class data using independent random sampling with replacement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 BalanceCascade\n",
    "\n",
    "- BalanceCascade algorithm takes a supervised learning approach that develops an ensemble of classifiers to systematically select which majority class examples to undersample.\n",
    "- For the first hypothesis of the ensemble, $H(1)$, consider a sampled set of majority class examples, $E$, such that $|E| = |S_{min}|$, and subject the ensemble to set $N = {E \\, U \\, S_{min}}$ to induce $H(1)$\n",
    "- Observing the results of $H(1)$, identify all $x_{i} \\, \\epsilon \\, N$ that are correctly classified as belonging to $S_{maj}$, call this collection $N_{maj}^{*}$\n",
    "- Then, since already have $H(1)$, it is reasonable to assume that $N_{maj}^{*}$ is somewhat redundant in $S_{maj}$ given that $H(1)$ is already trained.\n",
    "- Based on this remove set $N_{maj}^{*}$ from $S_{maj}$ and generate a new sampled set of majority class samples, $E$, such that $|E| = |S_{min}|$ and again subject the ensemble to $N = {E \\, U \\, S_{min}}$ to induce $H(2)$.\n",
    "- This procedure is iterated to a stopping criteria at which point a cascading combination scheme is used to form a final hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 KNN based Undersampling\n",
    "\n",
    "- Based on the characteristics of the given data distribution, four KNN undersampling methods were proposed namely, **NearMiss-1**, **NearMiss-2**, **NearMiss-3**, and the **most distant** method.\n",
    "- **NearMiss-1** method selects those majority examples whose average distance to the three closest minority class examples is the smallest.\n",
    "- **NearMiss-2** method selects the majority class examples whose average distance to the three farthest minority class examples is the smallest.\n",
    "- **NearMiss-3** selects a given number of the closest majority examples for each minority example to guarantee that every minority example is surrounded by some majority examples.\n",
    "- **Most-Distance** method selects the majority class examples whose average distance to the three closest minority class examples is the largest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 One-Sided Selection (OSS)\n",
    "\n",
    "- Selects a representative subset of the majority class $S_{maj}$, $E$ and combine it with the set of all minority examples $S_{min}$ to form a preliminary set N, $ \\; N = {E \\, U \\, S_{min}}$\n",
    "- This set N is further refined by using a data cleaning technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Synthetic Sampling with Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SMOTE (Synthetic Minority Oversampling Technique) creates artificial data based on the feature space similarities between existing minority examples.\n",
    "- For subset $S_{min} \\, \\epsilon \\, S$, consider the K-nearest neighbors for each example $x_{i} \\, \\epsilon \\, S_{min}$, for some specified integer K.\n",
    "- To create a synthetic sample, randomly select one of the K-nearest neighbors, then multiply the correspoding feature vector difference with a random number between **[0, 1]** and finally add this vector to $x_{i}$<br>\n",
    "$x_{new} = x_i + (\\dot{x} - x_{i})*\\delta$\n",
    "- Thus, the resulting synthetic instance is a point along the line segment joining $x_{i}$ under consideration and the randomly selected K-nearest neighbor $\\dot{x}$.\n",
    "- These synthetic samples help break the ties introduced by simple oversampling, and furthermore, augment the original data set in a manner that generally significantly improves learning.\n",
    "- Disadvantages\n",
    "    * Over-Generalization and variance\n",
    "- In SMOTE algorithm, problem of over-generalization is largely attributed to the way in which it creates synthetic samples.\n",
    "- SMOTE generates the same number of synthetic data samples for each original minority example and does so without consideration to neighboring examples, which increases the occurrence of overlapping between classes.\n",
    "- Various adaptive sampling methods have been proposed to overcome this limitation; some representative work includes the Borderline-SMOTE and Adaptive Synthtic Sampling (ADA-SYn) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Borderline-SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, determine set of nearest neighbours for each $x_{i} \\, \\epsilon \\, S_{min}$; call this set $S_{i:m-N \\, N}$,<br>$S_{i:m-N \\, N}\\subset S$\n",
    "- Next, for each $x_i$, identify the number of nearest neighbors that belongs to the majority class, i.e., $|S_{i:m-N \\, N} \\cap \\, S_{maj}|$\n",
    "- Finally, select those $x_i$, that satisfy:<br>\n",
    "$\\frac{m}{2} <= |S_{i:m-N \\, N} \\cap \\, S_{maj}| < m$\n",
    "- Equation suggests that only those $x_i$ that have more majority class neighbors than minority class neighbors are selected to form the set **“DANGER”**.\n",
    "- Therefore, the examples in **DANGER** represent the borderline minority class examples (the examples that are most likely to be misclassified). The DANGER set is then fed to the SMOTE algorithm to generate synthetic minority samples in the neighborhood of the borders.\n",
    "- If $|S_{i:m-N \\, N} \\cap \\, S_{maj}| = m$, i.e., if all of the $m$ nearest neighbors of $x_i$ are majority examples, then this $x_i$ is considered as noise and no synthetic examples are generated for it.<br><img src=\"imgs/Data Creation based on Borderline Instance.jpg\" width=\"800\" height=\"600\">\n",
    "- Major difference between Borderline-SMOTE and SMOTE is that SMOTE generates synthetic instances for each minority instance, while Borderline-SMOTE only generates synthetic instances for those minority examples **“closer”** to the border."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Adaptive Synthetic Sampling (ADASYN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Uses systematic method to adaptively create diferent amounts of synthetic data according to their distributions.\n",
    "- First calculating the number of synthetic data examples that needs to be generated for the entire minority class\n",
    "$ G = (|S_{maj}| - |S_{min}|) * \\beta$, where $\\beta \\, \\epsilon \\, [0, 1]$<br>is a parameter used to specify the desired balance level.\n",
    "- For each example $x_i \\, \\epsilon \\, S_{min}$, find the K-nearest neighbors according to the euclidean distance and calculate the ratio $\\gamma_i$ defined as<br>\n",
    "$\\gamma_i = \\frac{\\delta_i \\, / \\, {K}}{Z}, \\qquad i = 1,...,|S_{min}|$,<br> where $\\delta_i$ is the no. of examples in the K-nearest neighbors of $x_i$, that belongs to $S_{maj}$, and $Z$ is a normalization constant so that $\\gamma_i$ is a distribution function ($\\sum{\\gamma_i}=1$)\n",
    "- Then, determine the number of synthetic data samples that needs to be generated for each $x_i \\, \\epsilon \\, S_{min}$:\n",
    "$g_i = \\gamma_i * G$\n",
    "- Finally, for each $x_i \\, \\epsilon \\, S_{min}$, generate $g_i$ synthetic data samples using SMOTE.\n",
    "- The key idea of the ADASYN algorithm is to use a density distribution $\\gamma$ as a criterion to automatically decide the number of synthetic samples that need to be generated for each minority example by adaptively changing the weights of different minority examples to compensate for the skewed distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Sampling with Data Cleaning Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data Cleaning techniques like TOMEK Links is applied to remove the overalapping introduced by different sampling techniques.\n",
    "- TOMEK LINK: Pair of minimally distanced nearest neighbors of opposite classes $(x_i, x_j)$\n",
    "- For a TOMEK Link, either one of the instance is noise or both are near a border.\n",
    "- Can use Tomek links to “cleanup” unwanted overlapping between classes after synthetic sampling where all Tomek links are removed until all minimally distanced nearest neighbor pairs are of the same class. By removing overlapping examples, one can establish well-defined class clusters in the training set, which can, in turn, lead to well-defined classification rules for improved classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/tomek.jpg\" width=\"400\" height=\"200\">\n",
    "\n",
    "<center>Fig. (a) Original data set distribution. (b) Post-SMOTE data set.<br>(c) The identified Tomek Links. (d) The data set after removing Tomek links.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Edited Nearest Neighbor(ENN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTE: To be added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Repeated Edited Nearest Neighbor(RENN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTE: To be added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Cluster-Based Sampling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CBO (Cluster Based Oversampling) algorithm makes use of K-means clustering technique.\n",
    "- This procedure takes a random set of K examples from each cluster (for both classes) and computes the mean feature vector of these examples, which is designated as the cluster center.\n",
    "- Next, remaining examples are each assigned to a cluster, then all cluster means are updated and process is repreated until all examples are exhausted (i.e. only one cluster mean is essentially updated for each example).\n",
    "- Once all examples are exhausted, the CBO algorithm inflates all majority class clusters other than the largest by oversampling so that all majority class clusters are of the same size as the largest\n",
    "- CBO suggests that targeting within-class imbalance in tandem with the between-class imbalance is an effective strategy for imbalanced data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Bagging and Boosting Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTE: To be added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Integration of Sampling and Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **SMOTEBoost** introduces synthetic sampling at each boosting iteration. In this way, each successive classifier ensemble focuses more on the minority class.<br>Since each classifier ensemble is built on a different sampling of data, the final voted classifier is expected to have a broadened and well-defined decision region for the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr><hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
