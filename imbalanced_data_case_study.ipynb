{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr><hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Imbalanced Data: Underrepresented Data and severe class distribution skews\n",
    "\n",
    "- Most standard algorithms assume or expect balanced class distributions or equal misclassification costs. Therefore, when presented with complex imbalanced data sets, these algorithms fail to properly represent the distributive characteristics of the data and resultantly provide unfavorable accuracies across the classes of the data.\n",
    "\n",
    "- Some of the methods targeting the imbalanced learning -\n",
    "    * Sampling methods\n",
    "    * Cost-Sensitive learning methods\n",
    "    * Kernel-based learning methods\n",
    "    * Active Learning method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- More informative assessment metrics, such as the ROC curves, precision-recall curves, and cost curves, are necessary for conclusive evaluations of performance in the presence of imbalanced data.\n",
    "\n",
    "- **Instrinsic Variety:** Imbalances in data as a direct result of the nature of the dataspace.\n",
    "- **Exstrinsic Variety:** Imbalances in data is not directly related to the nature of the dataspace. Time and storage can result in imbalanced data.\n",
    "\n",
    "- **Relative Imbalance:** Imabalances in which minority class is in significant number but is rare relative to majority class.\n",
    "- **Imbalance due to rare instances (or, absoulte rarity)**\n",
    "\n",
    "- **Between-Class Imbalance:** Imbalance between the majority and minority class\n",
    "- **Within-Class Imbalance:** \n",
    "    * Minority/Majority concept may additionally contain a subconcept with limited instances, amounting to diverging degrees of classification difficulty and leading to within-class imbalance.\n",
    "    * It concerns itself with the distribution of representative data for subconcepts within a class.\n",
    "    * The no. of examples in the dominant clusters significantly outnumbers the examples in their respective subconcept clusters. \n",
    "    * Existence of within-class imbalances is closely interwined with the problem of small disjuncts, which has been shown to greatly depricate classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A classifier will attempt to learn a concept by creating multiple disjunct rules that describe the main concept.\n",
    "- In the case of homogeneous concepts, the classifier will generally create large disjuncts, i.e., rules that cover a large portion (cluster) of examples pertaining to the main concept.\n",
    "- However, in the case of heterogeneous concepts, small disjuncts, i.e., rules that cover a small cluster of examples pertaining to the main concept, arise as a direct result of underrepresented subconcepts.\n",
    "- Moreover, since classifiers attempt to learn both majority and minority concepts, the problem of small disjuncts is not only restricted to the minority concept.\n",
    "- On the contrary, small disjuncts of the majority class can arise from noisy misclassified minority class examples or underrepresented subconcepts.\n",
    "- However, because of the vast representation of majority class data, this occurrence is infrequent.\n",
    "- A more common scenario is that noise may influence disjuncts in the minority class, questioning whether these examples represent an actual subconcept or are merely attributed to noise, thus asking for the validity of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When standard learning algorithms are applied to imbalanced data, the induction rules that describe the minority concepts are often fewer and weaker than those of majority concepts, since the minority class is often both outnumbered and under-represented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decision trees use a recursive, top-down greedy search algorithm that uses a feature selection scheme (e.g., information gain) to select the best feature as the split criterion at each node of the tree; a successor (leaf) is then created for each of the possible values corresponding to the split feature.\n",
    "- The problem with decision tree algorithms in the presence of imbalanced data is\n",
    "    * First, successive partitioning of the dataspace results in fewer and fewer observations of minority class examples resulting in fewer leaves describing the minority concepts and successively weaker confidence estimates.\n",
    "    * Second, concepts that have dependencies on different feature space conjugations can go unlearned by the sparseness introduced through partitioning.\n",
    "- First issue correlates with the problems of relative and absolute imbalances, while the second issue best correlates with the between-class imbalance and the problem of high dimensionality.\n",
    "- After pruning, some regions will contain examples from both classes. A commonplace policy is to associate with each leaf of the label of the class that has majority in the corresponding region. Due to between-class imbalance, regions containing mixed binary labels will be most probably labelled as that of majority class, unless the algorithm is appropriately modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Studies have shown that for several base classifiers, a balanced data set provides improved overall classification performance compared to an imbalanced data set.\n",
    "- However, they do not imply that classifiers cannot learn from imbalanced data sets; on the contrary, studies have also shown that classifiers induced from certain imbalanced data sets are comparable to classifiers induced from the same data set balanced by sampling techniques.\n",
    "- Nevertheless, for most imbalanced data sets, the application of sampling techniques does indeed aid in improved classifier accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Three main approaches to learning from imbalanced data: \n",
    "    * **Data-level methods** that modify the collection of examples to balance distributions and/or remove difficult samples.\n",
    "    * **Algorithm-level methods** that directly modify existing learning algorithms to alleviate the bias towards majority objects and adapt them to the skewed distributions.\n",
    "    * **Hybrid methods** that combine the advantages of two previous groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One of the most interesting directions in binary imbalanced classification is the notion that imbalance-ratio is not the sole source of learning difficulties. Even if the disproportion is high, but both classes are well represented and come from non-overlapping distributions we may obtain good classification rates using canonical classifiers.\n",
    "- One of the prominent reasons of degradation of performance is due to within-class imbalance.\n",
    "- When dealing with small sample sizes, further reductions of noise / outliers in minority samples may be dangerous as how can one be sure that given example is a noise / outlier and not an inappropriatly sampled minority representative. Thus, removing such example may lead to wrong classification of potential new objects to appear in its neighborhood.\n",
    "- The current labelling method for identifying types of minority objects relies on k-nearest neighbors (with `k=5`) or kernel methods. This, however, strongly implies uniform distribution of data. So, there is a need for adaptive methods that will adjust the size of analyzed neighborhood according to local densities or chunk sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Random Oversampling and Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Undersampling aims to balance class distribution by randomly eliminating majority class examples.\n",
    "- Advantages:\n",
    "    * It can help improve run time and storage problems by reducing the number of training data samples when the training data set is huge.\n",
    "- Disadvantages:\n",
    "    * It can discard potentially useful information which could be important for building rule classifiers.\n",
    "    * The sample chosen by random under sampling may be a biased sample. And it will not be an accurate representative of the population. Thereby, resulting in inaccurate results with the actual test data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Oversampling increases the number of instances in the minority class by randomly replicating them in order to present a higher representation of the minority class in the sample.\n",
    "- Advantages:\n",
    "    * Unlike under sampling this method leads to no information loss.\n",
    "    * Outperforms under sampling\n",
    "- Disadvantages:\n",
    "    * It increases the likelihood of overfitting since it replicates the minority class events. <br>Oversampling simply appends replicated data to the original data set, resulting in multiple instances of certain examples becoming “tied”, leading to overfitting.<br>In particular, overfitting in oversampling occurs when classifiers produce multiple clauses in a rule for multiple copies of the same example which causes the rule to become too specific; although the training accuracy will be high in this scenario, the classification performance on the unseen testing data is generally far worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Informed Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Two examples of informed undersampling that have shown good results are **EasyEnsemble** and **BalanceCascade** algorithms.\n",
    "- The objective of these two methods is to overcome the deficiency of information loss introduced in the traditional random undersampling method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 EasyEnsemble\n",
    "\n",
    "- Implementation of EasyEnsemble is straightforward. It develops an ensemble learning system by independently sampling several subsets from the majority class and developing multiple classifiers based on the combination of each subset with the minority class data.\n",
    "- EasyEnsemble can be considered as an unsupervised learning that explores the majority class data using independent random sampling with replacement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 BalanceCascade\n",
    "\n",
    "- BalanceCascade algorithm takes a supervised learning approach that develops an ensemble of classifiers to systematically select which majority class examples to undersample.\n",
    "- For the first hypothesis of the ensemble, $H(1)$, consider a sampled set of majority class examples, $E$, such that $|E| = |S_{min}|$, and subject the ensemble to set $N = {E \\, U \\, S_{min}}$ to induce $H(1)$\n",
    "- Observing the results of $H(1)$, identify all $x_{i} \\, \\epsilon \\, N$ that are correctly classified as belonging to $S_{maj}$, call this collection $N_{maj}^{*}$\n",
    "- Then, since already have $H(1)$, it is reasonable to assume that $N_{maj}^{*}$ is somewhat redundant in $S_{maj}$ given that $H(1)$ is already trained.\n",
    "- Based on this remove set $N_{maj}^{*}$ from $S_{maj}$ and generate a new sampled set of majority class samples, $E$, such that $|E| = |S_{min}|$ and again subject the ensemble to $N = {E \\, U \\, S_{min}}$ to induce $H(2)$.\n",
    "- This procedure is iterated to a stopping criteria at which point a cascading combination scheme is used to form a final hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 KNN based Undersampling\n",
    "\n",
    "- Based on the characteristics of the given data distribution, four KNN undersampling methods were proposed namely, **NearMiss-1**, **NearMiss-2**, **NearMiss-3**, and the **most distant** method.\n",
    "- **NearMiss-1** method selects those majority examples whose average distance to the three closest minority class examples is the smallest.\n",
    "- **NearMiss-2** method selects the majority class examples whose average distance to the three farthest minority class examples is the smallest.\n",
    "- **NearMiss-3** selects a given number of the closest majority examples for each minority example to guarantee that every minority example is surrounded by some majority examples.\n",
    "- **Most-Distance** method selects the majority class examples whose average distance to the three closest minority class examples is the largest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 One-Sided Selection (OSS)\n",
    "\n",
    "- Selects a representative subset of the majority class $S_{maj}$, $E$ and combine it with the set of all minority examples $S_{min}$ to form a preliminary set N, $ \\; N = {E \\, U \\, S_{min}}$\n",
    "- This set N is further refined by using a data cleaning technique.\n",
    "- Heuristics to decide less reliable majority class examples:\n",
    "    * Class-Label noise\n",
    "    * Borderline examples that are close to the boundary between positive and negative regions\n",
    "    * Reduntant examples so that their part can be taken over by other examples\n",
    "    * Safe examples\n",
    "- Algorithm for OSS:\n",
    "    * Let $S$ be the original training set\n",
    "    * Initially, $C$ contains all the minority-class examples from $S$ and one randonly selected majority-class example.\n",
    "    * Classifiy S with 1-NN rule using the examples in $C$, and compare the assigned concept labels with the original ones. Move all misclassified examples into $C$ i.e. now consistent with $S$ while being smaller.\n",
    "    * Remove from $C$ all majority-class examples participating in Tomek links. This removes those majority-class that are believed borderline and/or noise. All minority-class examples are retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Synthetic Sampling with Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 SMOTE\n",
    "\n",
    "- SMOTE (Synthetic Minority Oversampling Technique) creates artificial data based on the feature space similarities between existing minority examples.\n",
    "- For subset $S_{min} \\, \\epsilon \\, S$, consider the K-nearest neighbors for each example $x_{i} \\, \\epsilon \\, S_{min}$, for some specified integer K.\n",
    "- To create a synthetic sample, randomly select one of the K-nearest neighbors, then multiply the correspoding feature vector difference with a random number between **[0, 1]** and finally add this vector to $x_{i}$<br>\n",
    "$x_{new} = x_i + (\\dot{x} - x_{i})*\\delta$\n",
    "- Thus, the resulting synthetic instance is a point along the line segment joining $x_{i}$ under consideration and the randomly selected K-nearest neighbor $\\dot{x}$.\n",
    "- These synthetic samples help break the ties introduced by simple oversampling, and furthermore, augment the original data set in a manner that generally significantly improves learning.\n",
    "- Disadvantages\n",
    "    * Over-Generalization and variance\n",
    "- In SMOTE algorithm, problem of over-generalization is largely attributed to the way in which it creates synthetic samples.\n",
    "- SMOTE generates the same number of synthetic data samples for each original minority example and does so without consideration to neighboring examples, which increases the occurrence of overlapping between classes.\n",
    "- Various adaptive sampling methods have been proposed to overcome this limitation; some representative work includes the Borderline-SMOTE and Adaptive Synthtic Sampling (ADA-SYn) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Borderline-SMOTE\n",
    "\n",
    "- First, determine set of nearest neighbours for each $x_{i} \\, \\epsilon \\, S_{min}$; call this set $S_{i:m-N \\, N}$,<br>$S_{i:m-N \\, N}\\subset S$\n",
    "- Next, for each $x_i$, identify the number of nearest neighbors that belongs to the majority class, i.e., $|S_{i:m-N \\, N} \\cap \\, S_{maj}|$\n",
    "- Finally, select those $x_i$, that satisfy:<br>\n",
    "$\\frac{m}{2} <= |S_{i:m-N \\, N} \\cap \\, S_{maj}| < m$\n",
    "- Equation suggests that only those $x_i$ that have more majority class neighbors than minority class neighbors are selected to form the set **“DANGER”**.\n",
    "- Therefore, the examples in **DANGER** represent the borderline minority class examples (the examples that are most likely to be misclassified). The DANGER set is then fed to the SMOTE algorithm to generate synthetic minority samples in the neighborhood of the borders.\n",
    "- If $|S_{i:m-N \\, N} \\cap \\, S_{maj}| = m$, i.e., if all of the $m$ nearest neighbors of $x_i$ are majority examples, then this $x_i$ is considered as noise and no synthetic examples are generated for it.<br><img src=\"imgs/Data Creation based on Borderline Instance.jpg\" width=\"800\" height=\"600\">\n",
    "- Major difference between Borderline-SMOTE and SMOTE is that SMOTE generates synthetic instances for each minority instance, while Borderline-SMOTE only generates synthetic instances for those minority examples **“closer”** to the border."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Adaptive Synthetic Sampling (ADASYN)\n",
    "\n",
    "- Uses systematic method to adaptively create diferent amounts of synthetic data according to their distributions.\n",
    "- First calculating the number of synthetic data examples that needs to be generated for the entire minority class\n",
    "$ G = (|S_{maj}| - |S_{min}|) * \\beta$, where $\\beta \\, \\epsilon \\, [0, 1]$<br>is a parameter used to specify the desired balance level.\n",
    "- For each example $x_i \\, \\epsilon \\, S_{min}$, find the K-nearest neighbors according to the euclidean distance and calculate the ratio $\\gamma_i$ defined as<br>\n",
    "$\\gamma_i = \\frac{\\delta_i \\, / \\, {K}}{Z}, \\qquad i = 1,...,|S_{min}|$,<br> where $\\delta_i$ is the no. of examples in the K-nearest neighbors of $x_i$, that belongs to $S_{maj}$, and $Z$ is a normalization constant so that $\\gamma_i$ is a distribution function ($\\sum{\\gamma_i}=1$)\n",
    "- Then, determine the number of synthetic data samples that needs to be generated for each $x_i \\, \\epsilon \\, S_{min}$:\n",
    "$g_i = \\gamma_i * G$\n",
    "- Finally, for each $x_i \\, \\epsilon \\, S_{min}$, generate $g_i$ synthetic data samples using SMOTE.\n",
    "- The key idea of the ADASYN algorithm is to use a density distribution $\\gamma$ as a criterion to automatically decide the number of synthetic samples that need to be generated for each minority example by adaptively changing the weights of different minority examples to compensate for the skewed distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sampling with Data Cleaning Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Tomek Links\n",
    "\n",
    "- Data Cleaning techniques like TOMEK Links is applied to remove the overalapping introduced by different sampling techniques.\n",
    "- TOMEK LINK: Pair of minimally distanced nearest neighbors of opposite classes $(x_i, x_j)$\n",
    "- For a TOMEK Link, either one of the instance is noise or both are near a border.\n",
    "- Can use Tomek links to “cleanup” unwanted overlapping between classes after synthetic sampling where all Tomek links are removed until all minimally distanced nearest neighbor pairs are of the same class.\n",
    "- By removing overlapping examples, one can establish well-defined class clusters in the training set, which can, in turn, lead to well-defined classification rules for improved classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"imgs/tomek.jpg\" width=\"400\" height=\"200\"></center>\n",
    "<center>Fig. (a) Original data set distribution. (b) Post-SMOTE data set.<br>(c) The identified Tomek Links. (d) The data set after removing Tomek links.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Edited Nearest Neighbor(ENN)\n",
    "\n",
    "[Asymptotic Properties of Nearest Neighbor Rules Using Edited Data](https://pdfs.semanticscholar.org/dea8/658ee4750ec6bb408a2281cf922cbb300a0a.pdf)\n",
    "\n",
    "- ENN aims to increase the classifier's generalization ability by removing noisy instances.\n",
    "- ENN is the base of algorithms like Repeated ENN (RENN) and All-KNN (ANN).\n",
    "- ENN removes all instances which have been misclassified by the KNN rule from the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"imgs/ENN with 1-NN classifier.jpg\" width=\"400\" height=\"200\"></center>\n",
    "<center>Fig. ENN editing with 1-NN classifier</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The idea of ENN relies on the fact that one can optimally eliminate outliers and possible overlap among classes from a given training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Repeated Edited Nearest Neighbor(RENN)\n",
    "\n",
    "- RENN applies the ENN algorithm repeatedly until all remaining instances have a majority of their neighbors with the same class, which continues to widen the gap between classes and smooth the decision boundary of ENN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 All-KNN(ANN)\n",
    "\n",
    "- ANN algorithm is similar with the iterative ENN with the only exception that the value k is increased after each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Cluster-Based Sampling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CBO (Cluster Based Oversampling) algorithm makes use of K-means clustering technique.\n",
    "- This procedure takes a random set of K examples from each cluster (for both classes) and computes the mean feature vector of these examples, which is designated as the cluster center.\n",
    "- Next, remaining examples are each assigned to a cluster, then all cluster means are updated and process is repreated until all examples are exhausted (i.e. only one cluster mean is essentially updated for each example).\n",
    "- Once all examples are exhausted, the CBO algorithm inflates all majority class clusters other than the largest by oversampling so that all majority class clusters are of the same size as the largest\n",
    "- Total number of majority class examples after the oversampling process, $N_{CBO} = |S_{maj}| + |E{maj}|$\n",
    "- CBO suggests that targeting within-class imbalance in tandem with the between-class imbalance is an effective strategy for imbalanced data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"imgs/CBO.jpg\" width=\"600\" height=\"300\"></center>\n",
    "<center>Fig. (a) Original data set distribution. (b) Distance vectors of examples\n",
    "and cluster means.<br>(c) Newly defined cluster means and cluster borders.\n",
    "(d) The data set after cluster-based oversampling method.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Bagging and Boosting Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTE: To be added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Integration of Sampling and Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **SMOTEBoost** introduces synthetic sampling at each boosting iteration. In this way, each successive classifier ensemble focuses more on the minority class.<br>Since each classifier ensemble is built on a different sampling of data, the final voted classifier is expected to have a broadened and well-defined decision region for the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Cost-Sensitive Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTE: To be added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from Extremely Imbalanced Dataset:\n",
    "- With such a high imbalance, the minority class is often poorly represented and lacks a clear structure. So, straightforward application of preprocessing methods that rely on relations between minority objects (like SMOTE) can actually deteriorate the classification performance.\n",
    "- Using randomized methods may also be inadvisable due to a high potential variance induced by the imbalance ratio.\n",
    "- Methods used in such case should be able to empower the minority class and predict or reconstruct a potential class structure.\n",
    "- Another possible way is to decompose original problem into a set of subproblems, each characterized by a reduced imbalanced ratio, and then approaches for highly-imbalanced ratio can be used. <br>This approach, however requires a two-way development:\n",
    "    * Algorithms for meaningful sub-division\n",
    "    * Algorithms for reconstruction of original extremely imbalanced task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from Imbalanced Big Data\n",
    "- Not only the increasing data volume can become prohibitive for existing methods, but also the nature of the problem can cause additional difficulties.\n",
    "- Following issues are important when designing new algorithms for learning from imbalanced data streams:\n",
    "    * To apply SMOTE-based techniques for massive datasets one either require new global-scale and efficient implementations, data partitioning methods that preserve relations between examples, or some global arbitrartion unit that will supervise the oversampling process.\n",
    "    * When dealing with imbalanced data we face one of two possible scenarios: when majority class is massive and minority class is of small sample size and when imbalance is present but representatives from both classes are abundant.<br>First issue is directly related to the problem of extreme imbalance.<br>Second issue is related to the observations that imbalance ratio may not be the main source of learning difficulties. This requires in-depth analysis of the structure of minority class and examples present there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from Imbalanced Data Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Whether dealing with stationary or evolving streams, require adaptive methods that are able to deal with skewed objects coming in real time (either as batches or online)\n",
    "- For changing streams, relationships between classes are no longer permanent.\n",
    "- Following issues are important when designing new algorithms for learning from imbalanced data streams:\n",
    "    * When objects from a given distribution will become less and less frequent, should we increase their importance along with increasing imbalance ratio?\n",
    "    * Using characteristics and struture of minority class is a promising direction for static imbalanced learning. But is it possible to adapt this approach to streaming data. Additionally,  even if samples arrive online their influence on minority class is local which may be useful hint for designing such systems.\n",
    "- Algorithms dedicated to tracking and analyzing the history of changes in minority class structure may also provide valuable insight into the evolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr><hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refernces\n",
    "1. Learning from Imbalanced Data<br>Haibo He, Member, IEEE, and Edwardo A. Garcia<br>IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 21, NO. 9, SEPTEMBER 2009\n",
    "\n",
    "2. Learning from imbalanced data: open challenges and future directions<br>Bartosz Krawczyk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- [Analytics Vidhya : Guide to handle imbalanced data](https://www.analyticsvidhya.com/blog/2017/03/imbalanced-data-classification/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr><hr><hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
